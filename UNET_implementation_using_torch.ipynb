{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNET implementation using torch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTpP_6DX8hDC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wHNUPEi8rQK"
      },
      "source": [
        "# Convolutional block:\n",
        " #   It follows a two 3x3 convolutional layer, each followed by a batch normalization and a relu activation.\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    \n",
        "    def forward(self, inputs):   #first layer it takes input\n",
        "        x = self.conv1(inputs)\n",
        "        #print(x.shape)\n",
        "        x = self.bn1(x)#batchnorm and relu don't change anything\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-lsND3385pK"
      },
      "source": [
        "    #now start encoder\n",
        "#   It consists of an conv_block followed by a max pooling.\n",
        "#   Here the number of filters doubles and the height and width half after every block.\n",
        "\n",
        "\n",
        "    class encoder_block(nn.Module): #base classs\n",
        "        def __init__(self, in_c, out_c): #child class constructor\n",
        "            super().__init__()\n",
        "\n",
        "            self.conv = conv_block(in_c, out_c)\n",
        "            self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        def forward(self, inputs):\n",
        "            x = self.conv(inputs)\n",
        "            p =self.pool(x)\n",
        "            return x, p"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKKv_wU2Aveb"
      },
      "source": [
        "# now start decoder\n",
        "#The decoder block begins with a transpose convolution, followed by a concatenation with the skip\n",
        "# connection from the encoder block. Next comes the conv_block.\n",
        "#Here the number filters decreases by half and the height and width doubles.\n",
        "\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "  def __init__(self, in_c, out_c):  #basically its a constructor\n",
        "    super().__init__()\n",
        "\n",
        "    self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "    self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "  def forward(self, inputs, skip):\n",
        "    x = self.up(inputs)\n",
        "    x = torch.cat([x, skip], axis=1)\n",
        "    #print(x.shape)\n",
        "    x = self.conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "       "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaGykoGznVEc"
      },
      "source": [
        " class build_unet(nn.Module):\n",
        "   def __init__(self):\n",
        "     super().__init__()\n",
        "     \n",
        "#encder\n",
        "     self.e1 = encoder_block(3, 64)\n",
        "     self.e2 = encoder_block(64, 128)\n",
        "     self.e3 = encoder_block(128, 256)\n",
        "     self.e4 = encoder_block(256, 512)\n",
        "\n",
        "\n",
        "     #bottleneck\n",
        "     self.b = conv_block(512, 1024)\n",
        "\n",
        "\n",
        "     #Decoder\n",
        "     self.d1 = decoder_block(1024, 512)\n",
        "     self.d2 = decoder_block(512, 256)\n",
        "     self.d3 = decoder_block(256, 128)\n",
        "     self.d4 = decoder_block(128, 64)\n",
        "\n",
        "     #Classifier \n",
        "     self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "\n",
        "\n",
        "   def forward(self, inputs):\n",
        "     #Encoder\n",
        "     s1, p1 = self.e1(inputs)\n",
        "     s2, p2 = self.e2(p1)\n",
        "     s3, p3 = self.e3(p2)\n",
        "     s4, p4 = self.e4(p3)\n",
        "\n",
        "     #Bottleneck\n",
        "     b = self.b(p4)\n",
        "\n",
        "     #Decoder\n",
        "     d1 = self.d1(b, s4)\n",
        "     d2 = self.d2(d1, s3)\n",
        "     d3 = self.d3(d2, s2)\n",
        "     d4 = self.d4(d3, s1)     \n",
        "\n",
        "     #Classifier\n",
        "     outputs = self.outputs(d4)\n",
        "\n",
        "     return outputs\n",
        "\n",
        "     #print(s1.shape, p1.shape)\n",
        "     #print(s2.shape, p2.shape)\n",
        "     #print(s3.shape, p3.shape)\n",
        "     #print(s4.shape, p4.shape)\n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        " #model = build_unet()\n",
        "  #model(inputs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4xUGY9L86pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a370e7-c2bc-4f75-bd60-1d194b5dad66"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  pass\n",
        "    # in tensoflow we write [b, h, w, c] in pytorch [b, c, h, w]\n",
        "  #  inputs = torch.randn((2, 3, 512, 512)) #specify shape\n",
        "\n",
        "    #e = encoder_block(3, 64)\n",
        "    #x, p = e(inputs)\n",
        "    #print(x.shape, p.shape)\n",
        "\n",
        "    #inputs = torch.randn((2, 64, 256, 256))\n",
        "    #skip = torch.randn((2, 32, 512, 512))\n",
        "    #d = decoder_block(64, 32)\n",
        "    #x = d(inputs,skip)\n",
        "    #print(x.shape)\n",
        "    #print(inputs, skip)\n",
        "  inputs = torch.randn((2, 3, 512, 512))\n",
        "  model = build_unet()\n",
        "  y = model(inputs)\n",
        "  print(y.shape)\n",
        "\n",
        "    #c = conv_block(3, 64)\n",
        "    #c(inputs)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 512, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NZ9ox0GAk7O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1UKQyDw-LYv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}